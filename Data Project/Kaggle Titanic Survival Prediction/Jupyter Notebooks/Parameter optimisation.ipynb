{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b635e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffc0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "X_train = pd.read_pickle(\"../Data/X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"../Data/X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"../Data/y_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31d0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the parameters leading to the highest score of a model\n",
    "\n",
    "def clf_performance(clf, model_name):\n",
    "    print(model_name)\n",
    "    print('Best Score: ' + str(clf.best_score_))\n",
    "    print('Best Parameters: ' + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4aecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Logistic Regression\n",
      "Best Score: 0.7953088300641149\n",
      "Best Parameters: {'C': 0.03359818286283781, 'max_iter': 2000, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "param_grid = {\n",
    "    'max_iter': [2000],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.logspace(-4,4,20),\n",
    "    'solver': ['liblinear']\n",
    "    \n",
    "}\n",
    "\n",
    "clf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "clf_lr_best = clf_lr.fit(X_train, y_train)\n",
    "clf_performance(clf_lr_best, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b82fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "KNN\n",
      "Best Score: 0.8155462451596522\n",
      "Best Parameters: {'algorithm': 'auto', 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {\n",
    "    'n_neighbors': [3,5,7,9],\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "    'p': [1,2]\n",
    "    \n",
    "}\n",
    "\n",
    "clf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_knn = clf_knn.fit(X_train, y_train)\n",
    "clf_performance(best_clf_knn, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248f6c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 55 candidates, totalling 275 fits\n",
      "SVC\n",
      "Best Score: 0.8234431536850124\n",
      "Best Parameters: {'C': 1, 'degree': 3, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "svc = SVC(probability = True)\n",
    "param_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1, .5, 1, 2, 5, 10], 'C': [.1,1,10,100,1000]},\n",
    "                                 {'kernel': ['linear'], 'C': [.1,1,10,100,1000]},\n",
    "                                {'kernel': ['poly'], 'degree':[2,3,4,5], 'C': [.1,1,10,100,1000]}\n",
    "                                ]\n",
    "\n",
    "clf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = 5, n_jobs = -1)\n",
    "best_clf_svc = clf_svc.fit(X_train, y_train)\n",
    "clf_performance(best_clf_svc, 'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed1d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Random Forest\n",
      "Best Score: 0.8312956262299245\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 1)\n",
    "param_grid =  {'n_estimators': [100,500,1000], \n",
    "                                  'bootstrap': [True,False],\n",
    "                                  'max_depth': [3,5,10,20,50,75,100,None],\n",
    "                                  'max_features': ['auto','sqrt'],\n",
    "                                  'min_samples_leaf': [1,2,4,10],\n",
    "                                  'min_samples_split': [2,5,10]}\n",
    "                                  \n",
    "clf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_rf_rnd = clf_rf_rnd.fit(X_train,y_train)\n",
    "clf_performance(best_clf_rf_rnd,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be984cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "best_rf = best_clf_rf_rnd.best_estimator_.fit(X_train, y_train)\n",
    "feat_importances = pd.Series(best_rf.feature_importances_, index = X_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87a6bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicholas\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.78627563        nan        nan        nan 0.78402844\n",
      "        nan 0.61754586 0.78627563 0.61754586 0.78518377 0.61754586\n",
      " 0.81217546 0.83351108        nan        nan 0.83915445        nan\n",
      "        nan        nan 0.80091411        nan        nan        nan\n",
      " 0.61754586 0.83353647        nan        nan        nan 0.81435917\n",
      " 0.82228147        nan        nan        nan        nan 0.80768108\n",
      " 0.78402209 0.78627563        nan        nan 0.80657018 0.77841046\n",
      " 0.64451216 0.80990922        nan        nan        nan 0.82005332\n",
      " 0.81552085        nan 0.82228147 0.81668254 0.78627563        nan\n",
      " 0.81215641 0.61754586 0.77841046 0.78627563        nan 0.78627563\n",
      " 0.61754586 0.79640069 0.81103282        nan 0.78627563        nan\n",
      "        nan 0.78853552 0.825684          nan 0.79867962        nan\n",
      "        nan        nan        nan 0.82004698        nan        nan\n",
      "        nan        nan 0.61754586        nan 0.79191265 0.79078271\n",
      " 0.78627563        nan 0.61754586 0.81331175 0.82565226 0.8369136\n",
      " 0.83015933        nan        nan        nan        nan 0.79641338\n",
      "        nan 0.84142703        nan 0.79753698        nan        nan\n",
      " 0.80655748        nan        nan        nan 0.83465372        nan\n",
      " 0.80203771 0.61754586 0.78627563        nan 0.78065765 0.83579001\n",
      "        nan        nan        nan        nan 0.61754586 0.61754586\n",
      " 0.61754586 0.83466006        nan        nan 0.78852282 0.8166381\n",
      "        nan        nan        nan 0.80541484        nan 0.82341776\n",
      "        nan 0.81666984        nan        nan        nan 0.78519012\n",
      "        nan        nan        nan        nan        nan 0.61754586\n",
      "        nan        nan 0.78404748        nan        nan 0.82449692\n",
      "        nan 0.61754586        nan        nan        nan        nan\n",
      " 0.78965911 0.78627563        nan        nan 0.78627563        nan\n",
      " 0.78627563 0.8290167  0.61754586        nan 0.79751793        nan\n",
      "        nan 0.82001524        nan 0.61754586 0.61754586        nan\n",
      "        nan        nan        nan        nan        nan 0.83014664\n",
      "        nan 0.78627563        nan 0.84029074        nan 0.83916079\n",
      "        nan 0.77278614 0.81213737        nan 0.61754586 0.78627563\n",
      "        nan 0.83240018        nan        nan        nan 0.83130197\n",
      " 0.83464102        nan 0.80653844        nan        nan        nan\n",
      "        nan        nan 0.80425316        nan 0.78627563        nan\n",
      "        nan 0.81216276        nan 0.61754586        nan 0.81666984\n",
      "        nan        nan        nan        nan 0.83017203        nan\n",
      "        nan        nan        nan 0.61754586        nan 0.80992827\n",
      " 0.64451216 0.82000254 0.7930172  0.81889799        nan 0.78627563\n",
      " 0.83130197        nan 0.8133181         nan        nan        nan\n",
      "        nan 0.83355551 0.7908081  0.78515838        nan 0.83801816\n",
      " 0.81331175 0.61754586 0.61754586 0.77954041        nan 0.82115787\n",
      " 0.77503967        nan        nan        nan 0.79412175        nan\n",
      " 0.78627563        nan 0.81779344        nan 0.78627563 0.83693265\n",
      "        nan        nan        nan        nan 0.83693265        nan\n",
      " 0.61754586        nan        nan        nan 0.82230051        nan\n",
      " 0.82229417        nan 0.8267949  0.81778074        nan        nan\n",
      "        nan 0.81777439        nan        nan        nan        nan\n",
      " 0.83242557        nan 0.83125754 0.61754586        nan 0.82566495\n",
      "        nan        nan 0.8380499         nan        nan        nan\n",
      " 0.83353647        nan        nan        nan 0.78403479 0.7919317\n",
      " 0.78627563        nan 0.81894242        nan        nan 0.78627563\n",
      "        nan 0.84137625 0.80990288        nan 0.78401574        nan\n",
      " 0.82793754 0.61754586        nan        nan 0.61754586        nan\n",
      "        nan 0.61754586 0.8369263         nan 0.82567765        nan\n",
      "        nan        nan 0.61754586 0.83466006 0.83241922 0.61754586\n",
      "        nan        nan 0.78627563        nan 0.64451216        nan\n",
      "        nan        nan 0.83017203        nan 0.61754586 0.83355551\n",
      " 0.61754586        nan        nan 0.77279248        nan 0.77616327\n",
      " 0.82115153 0.82452866        nan        nan 0.78627563        nan\n",
      " 0.82904844        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.61754586        nan 0.61754586 0.61754586\n",
      " 0.80312956 0.77728687 0.82680124        nan 0.80654479 0.61754586\n",
      "        nan        nan 0.61754586        nan        nan 0.61754586\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan 0.78627563        nan 0.61754586        nan        nan\n",
      " 0.81894242 0.61754586 0.84029074        nan 0.83014664 0.83801181\n",
      " 0.82791849 0.61754586 0.83125754        nan        nan        nan\n",
      " 0.8279058         nan        nan        nan 0.79643243        nan\n",
      "        nan        nan 0.82791214 0.61754586        nan 0.82567765\n",
      " 0.82793754 0.78742462        nan        nan 0.79977782 0.79979686\n",
      "        nan        nan 0.79191265 0.61754586        nan 0.83352377\n",
      "        nan        nan        nan 0.75256776 0.81442265 0.82228147\n",
      "        nan        nan        nan 0.8290167  0.81666984        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.61754586        nan\n",
      "        nan 0.80766203        nan        nan        nan 0.82117692\n",
      " 0.77502698        nan        nan        nan 0.61754586        nan\n",
      "        nan 0.61754586 0.82002793 0.78627563 0.78741192 0.82901035\n",
      " 0.81668254 0.82677585        nan        nan        nan 0.82341776\n",
      "        nan 0.83467276 0.83013394        nan 0.83239383 0.79078271\n",
      "        nan        nan        nan        nan 0.61754586        nan\n",
      "        nan        nan 0.82000254 0.8380245  0.83125754 0.61754586\n",
      "        nan 0.79191265        nan        nan 0.78402209 0.80316765\n",
      "        nan        nan 0.7806513  0.82453501 0.82230051 0.81778074\n",
      " 0.80653844 0.82680124 0.81554625 0.61754586        nan        nan\n",
      " 0.82561417 0.80768108        nan 0.80651939        nan        nan\n",
      " 0.8133181  0.83693265        nan        nan 0.82675046 0.61754586\n",
      " 0.8267695         nan        nan 0.82115787        nan        nan\n",
      " 0.82564591 0.61754586 0.80875389 0.61754586        nan 0.82678855\n",
      " 0.82901035        nan 0.61754586 0.61754586 0.79526439 0.77277979\n",
      " 0.83019107        nan        nan        nan        nan 0.78627563\n",
      "        nan        nan        nan 0.8278931  0.80425316 0.61754586\n",
      " 0.61754586 0.77391608        nan 0.8267949         nan 0.82228147\n",
      "        nan        nan 0.83238748 0.83017203        nan        nan\n",
      "        nan        nan 0.82564591 0.78627563        nan 0.80092046\n",
      " 0.78627563 0.81554625        nan        nan        nan        nan\n",
      " 0.78627563        nan        nan 0.61754586        nan 0.61754586\n",
      " 0.78627563 0.83241287 0.78627563        nan 0.61754586 0.79643877\n",
      "        nan        nan 0.83238113 0.61754586 0.61754586        nan\n",
      " 0.79527074        nan        nan 0.78627563        nan 0.79754333\n",
      " 0.61754586        nan 0.8133054         nan 0.61754586 0.61754586\n",
      " 0.83354917        nan        nan 0.80989653 0.78967816        nan\n",
      " 0.80992827        nan 0.61754586        nan        nan 0.82680124\n",
      " 0.81665714        nan        nan        nan 0.61754586 0.80766203\n",
      "        nan        nan 0.77503967        nan        nan        nan\n",
      " 0.80315495 0.83129563 0.61754586 0.61754586        nan 0.80763664\n",
      " 0.78067035        nan 0.82002158 0.61754586        nan        nan\n",
      " 0.78966546        nan        nan 0.79751159        nan        nan\n",
      " 0.82901035        nan        nan 0.79754333        nan        nan\n",
      "        nan        nan 0.83241287        nan        nan 0.78627563\n",
      " 0.82566495 0.79641338        nan 0.82117057 0.82228782 0.78065765\n",
      " 0.83356821 0.80653844 0.61754586 0.61754586        nan 0.83237479\n",
      " 0.82226877 0.78627563        nan 0.81215007 0.77728687        nan\n",
      "        nan 0.61754586        nan 0.83575827        nan 0.80879832\n",
      " 0.83466641        nan 0.78627563        nan 0.81892338 0.77279248\n",
      " 0.82903574 0.77728687        nan        nan 0.82005332        nan\n",
      "        nan 0.80314861 0.82680124        nan        nan 0.82003428\n",
      " 0.80090776 0.829004   0.80990288        nan        nan 0.81216276\n",
      " 0.77391608        nan 0.8155272  0.82114518 0.81216276 0.80990288\n",
      " 0.83464102        nan        nan        nan 0.74919698 0.78515203\n",
      " 0.78627563 0.78627563 0.61754586        nan        nan        nan\n",
      " 0.82455405 0.78852282        nan 0.75237732 0.84030343        nan\n",
      " 0.61754586        nan        nan        nan        nan        nan\n",
      "        nan 0.78627563        nan 0.80314861 0.78740557        nan\n",
      "        nan        nan 0.80879198        nan        nan 0.82567765\n",
      "        nan        nan 0.83129563        nan        nan 0.79302355\n",
      " 0.81438456 0.78627563 0.81444169 0.82338602 0.78404114        nan\n",
      " 0.82788675        nan        nan 0.82449692        nan 0.8279058\n",
      "        nan        nan 0.83693265 0.8020504  0.81889799 0.8267949\n",
      "        nan 0.61754586 0.78627563 0.79191265 0.68159081        nan\n",
      "        nan 0.82114518 0.78741827        nan 0.83466641        nan\n",
      "        nan        nan 0.61754586 0.61754586 0.84029074        nan\n",
      "        nan 0.83579001 0.83241287        nan 0.83354917        nan\n",
      "        nan        nan        nan 0.83017203 0.82567765 0.61754586\n",
      " 0.61754586 0.78627563 0.82452231        nan        nan        nan\n",
      "        nan        nan 0.71548911 0.84141433        nan 0.61754586\n",
      "        nan        nan        nan 0.61754586        nan        nan\n",
      "        nan        nan 0.81325462 0.77616327 0.74919698        nan\n",
      "        nan 0.61754586        nan 0.61754586        nan        nan\n",
      "        nan        nan 0.83241922 0.78627563        nan        nan\n",
      "        nan 0.77507141 0.83688186        nan        nan 0.61754586\n",
      "        nan        nan 0.61754586        nan        nan        nan\n",
      "        nan        nan        nan 0.83801181        nan 0.61754586\n",
      "        nan 0.61754586        nan 0.77391608 0.7941281  0.82112613\n",
      "        nan 0.61754586 0.83688821        nan 0.81218815        nan\n",
      " 0.82342411 0.80090776 0.80205675 0.61754586        nan 0.78741192\n",
      "        nan 0.79981591        nan 0.8267695  0.79979052        nan\n",
      "        nan        nan 0.81778709        nan        nan        nan\n",
      "        nan 0.81329271 0.82115787        nan        nan 0.80318035\n",
      "        nan 0.83012759 0.83240653        nan 0.81218815        nan\n",
      "        nan        nan        nan 0.82899765        nan 0.8155272\n",
      "        nan 0.61754586 0.79641973 0.79641973        nan        nan\n",
      "        nan 0.61754586        nan 0.81328636 0.78853552 0.81552085\n",
      "        nan        nan        nan        nan 0.82454136 0.81888529\n",
      "        nan 0.67841046        nan        nan        nan        nan\n",
      "        nan 0.83128293        nan 0.78627563        nan 0.61754586\n",
      "        nan 0.77503967        nan 0.81891703        nan 0.61754586\n",
      "        nan        nan 0.61754586        nan        nan 0.61754586\n",
      "        nan 0.8133054         nan 0.61754586        nan 0.77728687\n",
      "        nan 0.81660636 0.82902939        nan 0.79866057 0.83017203\n",
      "        nan        nan        nan        nan 0.83354282        nan\n",
      "        nan 0.61754586        nan        nan        nan 0.81888529\n",
      " 0.83015299 0.61754586 0.61754586 0.61754586        nan 0.61754586\n",
      "        nan        nan        nan        nan 0.81893608 0.8358027\n",
      " 0.81550816        nan        nan 0.82112613 0.79309973 0.81667619\n",
      " 0.80540215        nan 0.81779344 0.61754586]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB\n",
      "Best Score: 0.8414270297721069\n",
      "Best Parameters: {'subsample': 0.8, 'sampling_method': 'uniform', 'reg_lambda': 2, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 0.1, 'max_depth': 10, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "xgb = XGBClassifier(random_state = 1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 250, 500,1000],\n",
    "    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n",
    "    'max_depth': [2, 5, 10, 15, 20, 25, None],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n",
    "    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n",
    "    'gamma':[0,.01,.1,1,10,100],\n",
    "    'min_child_weight':[0,.01,0.1,1,10,100],\n",
    "    'sampling_method': ['uniform', 'gradient_based']\n",
    "}\n",
    "\n",
    "#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n",
    "#clf_performance(best_clf_xgb,'XGB')\n",
    "clf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_xgb_rnd = clf_xgb_rnd.fit(X_train,y_train)\n",
    "clf_performance(best_clf_xgb_rnd,'XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca22ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "XGB\n",
      "Best Score: 0.8425315812861042\n",
      "Best Parameters: {'colsample_bytree': 0.85, 'gamma': 0.5, 'learning_rate': 0.5, 'max_depth': None, 'min_child_weight': 0.01, 'n_estimators': 450, 'reg_alpha': 1, 'reg_lambda': 10, 'sampling_method': 'uniform', 'subsample': 0.65}\n"
     ]
    }
   ],
   "source": [
    "# Find the particular parameters that lead to the highest score\n",
    "\n",
    "xgb = XGBClassifier(random_state = 1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [450,500,550],\n",
    "    'colsample_bytree': [0.75,0.8,0.85],\n",
    "    'max_depth': [None],\n",
    "    'reg_alpha': [1],\n",
    "    'reg_lambda': [2, 5, 10],\n",
    "    'subsample': [0.55, 0.6, .65],\n",
    "    'learning_rate':[0.5],\n",
    "    'gamma':[.5,1,2],\n",
    "    'min_child_weight':[0.01],\n",
    "    'sampling_method': ['uniform']\n",
    "}\n",
    "\n",
    "clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_xgb = clf_xgb.fit(X_train,y_train)\n",
    "clf_performance(best_clf_xgb,'XGB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1662b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction based on the best xgb model\n",
    "\n",
    "y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test).astype(int)\n",
    "xgb_submission = {'PassengerId': X_test.index, 'Survived': y_hat_xgb}\n",
    "submission_xgb = pd.DataFrame(data = xgb_submission)\n",
    "submission_xgb.to_csv('../Submissions/sgb_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22b7816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting_clf_hard:  [0.80898876 0.79213483 0.85955056 0.79775281 0.85310734]\n",
      "voting_clf_hard:  0.8223068621849807\n",
      "voting_clf_soft:  [0.79213483 0.80898876 0.82022472 0.79213483 0.8700565 ]\n",
      "voting_clf_soft:  0.8178315241541293\n",
      "voting_clf_all:  [0.81460674 0.80898876 0.84831461 0.79775281 0.86440678]\n",
      "voting_clf_all:  0.8268139402018664\n"
     ]
    }
   ],
   "source": [
    "# Use a voting classifier to find the best scores using all the different models\n",
    "\n",
    "best_lr = clf_lr_best.best_estimator_\n",
    "best_knn = best_clf_knn.best_estimator_\n",
    "best_svc = best_clf_svc.best_estimator_\n",
    "best_rf = best_clf_rf_rnd.best_estimator_\n",
    "best_xgb = best_clf_xgb.best_estimator_\n",
    "\n",
    "voting_clf_hard = VotingClassifier(estimators = [('knn', best_knn), ('rf', best_rf), ('svc', best_svc)], voting = 'hard')\n",
    "voting_clf_soft = VotingClassifier(estimators = [('knn', best_knn), ('svc', best_svc), ('rf', best_rf)], voting = 'soft')\n",
    "\n",
    "voting_clf_all = VotingClassifier(estimators = [('knn', best_knn), ('svc', best_svc), ('rf', best_rf), ('xgb', best_xgb), ('lr', best_lr)\n",
    "    ], voting = 'soft')\n",
    "\n",
    "print('voting_clf_hard: ', cross_val_score(voting_clf_hard, X_train, y_train, cv = 5))\n",
    "print('voting_clf_hard: ', cross_val_score(voting_clf_hard, X_train, y_train, cv = 5).mean())\n",
    "\n",
    "print('voting_clf_soft: ', cross_val_score(voting_clf_soft, X_train, y_train, cv = 5))\n",
    "print('voting_clf_soft: ', cross_val_score(voting_clf_soft, X_train, y_train, cv = 5).mean())\n",
    "\n",
    "print('voting_clf_all: ', cross_val_score(voting_clf_all, X_train, y_train, cv = 5))\n",
    "print('voting_clf_all: ', cross_val_score(voting_clf_all, X_train, y_train, cv = 5).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40edfce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "VC Weights\n",
      "Best Score: 0.8223259061766013\n",
      "Best Parameters: {'weights': [1, 1, 2]}\n"
     ]
    }
   ],
   "source": [
    "# Use a voting classifier to find the best scores using all the different models with different weights for models\n",
    "\n",
    "params = {'weights': [[1,1,1], [1,2,1], [1,1,2], [2,1,1], [2,2,1], [2,1,2], [1,2,2]]}\n",
    "\n",
    "vote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv=5, verbose = True, n_jobs = -1)\n",
    "best_clf_weight = vote_weight.fit(X_train, y_train)\n",
    "clf_performance(best_clf_weight, 'VC Weights')\n",
    "voting_clf_sub = best_clf_weight.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fb84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scores for each voting classifier\n",
    "\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "voting_clf_all.fit(X_train, y_train)\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "y_hat_vc_hard = voting_clf_hard.predict(X_test).astype(int)\n",
    "y_hat_rf = best_rf.predict(X_test).astype(int)\n",
    "y_hat_vc_soft = voting_clf_soft.predict(X_test).astype(int)\n",
    "y_hat_vc_all = voting_clf_all.predict(X_test).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7047b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submissions based on the different voting classifiers\n",
    "\n",
    "final_data = {'PassengerId': X_test.index, 'Survived': y_hat_rf}\n",
    "submission = pd.DataFrame(data=final_data)\n",
    "\n",
    "final_data_2 = {'PassengerId': X_test.index, 'Survived': y_hat_vc_hard}\n",
    "submission_2 = pd.DataFrame(data=final_data_2)\n",
    "\n",
    "final_data_3 = {'PassengerId': X_test.index, 'Survived': y_hat_vc_soft}\n",
    "submission_3 = pd.DataFrame(data=final_data_3)\n",
    "\n",
    "final_data_4 = {'PassengerId': X_test.index, 'Survived': y_hat_vc_all}\n",
    "submission_4 = pd.DataFrame(data=final_data_4)\n",
    "\n",
    "final_data_comp = {'PassengerId': X_test.index, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  }\n",
    "comparison = pd.DataFrame(data=final_data_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b21bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the voting classifiers\n",
    "\n",
    "comparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\n",
    "comparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\n",
    "comparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc60200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    396\n",
       "1     22\n",
       "Name: difference_hard_all, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison.difference_hard_all.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd17b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80d5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
